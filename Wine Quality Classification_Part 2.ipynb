{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a134622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas # https://pandas.pydata.org/\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# pretty tables\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy for numerical computing\n",
    "import numpy # https://numpy.org/\n",
    "\n",
    "# MatPlotLib + Seaborn for visualization\n",
    "import matplotlib.pyplot as pl  # https://matplotlib.org/\n",
    "import seaborn as sns   # https://seaborn.pydata.org/\n",
    "\n",
    "# assessment\n",
    "from sklearn import model_selection # for model comparisons\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# data preprocessing / feature selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd18839",
   "metadata": {},
   "source": [
    "* **Load Data**\n",
    "\n",
    "* **Separate the data into training and testing datasets**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b90c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file ...\n",
      "done \n",
      "\n",
      "Removing rows with missing data ...\n",
      "done \n",
      "\n",
      "Partitioning data into parts: formative (for development) and summative (for testing) ...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data from file ...')  # Now let's load the data\n",
    "dataset = pandas.read_csv('winequality-white.csv') # default is header=infer, change if column names are not in first row\n",
    "print('done \\n')\n",
    "\n",
    "print('Removing rows with missing data ...')  # Make things simple\n",
    "dataset = dataset.dropna()  # default is to drop any row that contains at least one missing value\n",
    "print('done \\n')\n",
    "\n",
    "\n",
    "X_name = [ 'fixed acidity', 'volatile acidity', 'citric acid' , 'residual sugar', 'chlorides', \n",
    "          'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol' ] \n",
    "y_name = 'quality'\n",
    "X = dataset[X_name]   \n",
    "y = dataset[y_name] \n",
    "\n",
    "\n",
    "# setting the seed allows for repeatability\n",
    "seed = 42 \n",
    "\n",
    "print('Partitioning data into parts: formative (for development) and summative (for testing) ...')\n",
    "test_size = 0.20   # means 20 percent\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print('done \\n')\n",
    "\n",
    "\n",
    "# standardization \n",
    "norm = StandardScaler()\n",
    "\n",
    "X_train = norm.fit_transform(X_train)\n",
    "X_test= norm.transform( X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30dc8bc",
   "metadata": {},
   "source": [
    "# <font color='red'>Part 1</font> Train and tune the MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea7feb",
   "metadata": {},
   "source": [
    "* **First Combination**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963e703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.30755799\n",
      "Iteration 2, loss = 1.12818167\n",
      "Iteration 3, loss = 1.08311290\n",
      "Iteration 4, loss = 1.08268734\n",
      "Iteration 5, loss = 1.05676321\n",
      "Iteration 6, loss = 1.06623657\n",
      "Iteration 7, loss = 1.04764248\n",
      "Iteration 8, loss = 1.04253996\n",
      "Iteration 9, loss = 1.02248645\n",
      "Iteration 10, loss = 1.02081959\n",
      "Training set score: 0.547984\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Design the classifier neural network\n",
    "mlp_1 = MLPClassifier(hidden_layer_sizes=(50,), # one hidden layer with 50 neurons\n",
    "                    activation = 'relu',  # ReLU is the default option\n",
    "                    # solver='adam',  # default is Adam\n",
    "                    alpha=1e-4,  # regulariztion parameter, set to default=0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                    learning_rate_init=.1 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                    max_iter=10,  # number of epochs, default=200\n",
    "                    random_state=42,\n",
    "                    verbose=10, \n",
    "                    )\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp_1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp_1.score(X_train, y_train))\n",
    "print('Done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf5db3",
   "metadata": {},
   "source": [
    "* **Second Combination**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f33f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.25717287\n",
      "Iteration 2, loss = 1.13190938\n",
      "Iteration 3, loss = 1.10248209\n",
      "Iteration 4, loss = 1.08688077\n",
      "Iteration 5, loss = 1.07586170\n",
      "Iteration 6, loss = 1.06316864\n",
      "Iteration 7, loss = 1.05434857\n",
      "Iteration 8, loss = 1.06500985\n",
      "Iteration 9, loss = 1.05289737\n",
      "Iteration 10, loss = 1.07007771\n",
      "Training set score: 0.564829\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Design the classifier neural network\n",
    "mlp_2 = MLPClassifier(hidden_layer_sizes=(70,10), # one hidden layer with 50 neurons\n",
    "                    activation = 'tanh',  # ReLU is the default option\n",
    "                    # solver='adam',  # default is Adam\n",
    "                    alpha=1e-4,  # regulariztion parameter, set to default=0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                    learning_rate_init=.1 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                    max_iter=10,  # number of epochs, default=200\n",
    "                    random_state=42,\n",
    "                    verbose=10, \n",
    "                    )\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp_2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp_2.score(X_train, y_train))\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3acfa",
   "metadata": {},
   "source": [
    "* **Third Combination**\n",
    "two hidden layers, 100 neurons, activation =relu, learning rate=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22361621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35094347\n",
      "Iteration 2, loss = 1.17126506\n",
      "Iteration 3, loss = 1.13445636\n",
      "Iteration 4, loss = 1.11696921\n",
      "Iteration 5, loss = 1.12174861\n",
      "Iteration 6, loss = 1.12211773\n",
      "Iteration 7, loss = 1.11820783\n",
      "Iteration 8, loss = 1.11775899\n",
      "Iteration 9, loss = 1.11723379\n",
      "Iteration 10, loss = 1.12026474\n",
      "Training set score: 0.521184\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Design the classifier neural network\n",
    "mlp_3 = MLPClassifier(hidden_layer_sizes=(60,3), # one hidden layer with 50 neurons\n",
    "                    activation = 'identity',  # ReLU is the default option\n",
    "                    # solver='adam',  # default is Adam\n",
    "                    alpha=1e-4,  # regulariztion parameter, set to default=0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                    learning_rate_init=.1 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                    max_iter=10,  # number of epochs, default=200\n",
    "                    random_state=42,\n",
    "                    verbose=10, \n",
    "                    )\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp_3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp_3.score(X_train, y_train))\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9f757",
   "metadata": {},
   "source": [
    "#  <font color='red'>Part 2</font>\n",
    "\n",
    "* **Study and describe the performance impact of varying at least three different combinations of optimizer parameter values**\n",
    "\n",
    "Here we choose the second combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc0bdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.29980690\n",
      "Iteration 2, loss = 1.09971481\n",
      "Iteration 3, loss = 1.07319292\n",
      "Iteration 4, loss = 1.05663549\n",
      "Iteration 5, loss = 1.04281709\n",
      "Iteration 6, loss = 1.03171139\n",
      "Iteration 7, loss = 1.02828198\n",
      "Iteration 8, loss = 1.02073831\n",
      "Iteration 9, loss = 1.01678975\n",
      "Iteration 10, loss = 1.00534610\n",
      "Iteration 11, loss = 0.99550122\n",
      "Iteration 12, loss = 0.99183760\n",
      "Iteration 13, loss = 0.98713338\n",
      "Iteration 14, loss = 0.98308534\n",
      "Iteration 15, loss = 0.97637727\n",
      "Iteration 16, loss = 0.97130720\n",
      "Iteration 17, loss = 0.97004320\n",
      "Iteration 18, loss = 0.96036115\n",
      "Iteration 19, loss = 0.95053406\n",
      "Iteration 20, loss = 0.93995578\n",
      "Iteration 21, loss = 0.93865588\n",
      "Iteration 22, loss = 0.93129942\n",
      "Iteration 23, loss = 0.92587846\n",
      "Iteration 24, loss = 0.92078958\n",
      "Iteration 25, loss = 0.91836592\n",
      "Iteration 26, loss = 0.91942653\n",
      "Iteration 27, loss = 0.90897057\n",
      "Iteration 28, loss = 0.90020841\n",
      "Iteration 29, loss = 0.89667108\n",
      "Iteration 30, loss = 0.89626852\n",
      "Iteration 31, loss = 0.89488263\n",
      "Iteration 32, loss = 0.88282757\n",
      "Iteration 33, loss = 0.87394220\n",
      "Iteration 34, loss = 0.86345381\n",
      "Iteration 35, loss = 0.86187005\n",
      "Iteration 36, loss = 0.85382128\n",
      "Iteration 37, loss = 0.85576124\n",
      "Iteration 38, loss = 0.85190413\n",
      "Iteration 39, loss = 0.84651927\n",
      "Iteration 40, loss = 0.83540885\n",
      "Iteration 41, loss = 0.82508376\n",
      "Iteration 42, loss = 0.82413652\n",
      "Iteration 43, loss = 0.82157865\n",
      "Iteration 44, loss = 0.81285801\n",
      "Iteration 45, loss = 0.80590283\n",
      "Iteration 46, loss = 0.80169966\n",
      "Iteration 47, loss = 0.80220111\n",
      "Iteration 48, loss = 0.79586972\n",
      "Iteration 49, loss = 0.79258280\n",
      "Iteration 50, loss = 0.78570141\n",
      "Training set score: 0.689127\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Design the classifier neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(70,10), # one hidden layer with 50 neurons\n",
    "                    activation = 'tanh',  # ReLU is the default option\n",
    "                    # solver='sgd',  # default is Adam\n",
    "                    alpha=1e-4,  # regulariztion parameter, set to default=0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                    learning_rate_init=.01 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                    max_iter=50,  # number of epochs, default=200\n",
    "                    random_state=42,\n",
    "                    verbose=10, \n",
    "                    )\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac426a86",
   "metadata": {},
   "source": [
    "Here we set smaller learning which requires more training epochs. As we can see, loss values is decreasing smoothly and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d2100",
   "metadata": {},
   "source": [
    "#  <font color='red'>Part 3</font>\n",
    "\n",
    "* **Test the performance of the best MLPClassifier**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75a3f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         5\n",
      "           4       0.43      0.24      0.31        25\n",
      "           5       0.58      0.65      0.61       291\n",
      "           6       0.57      0.59      0.58       432\n",
      "           7       0.52      0.48      0.50       192\n",
      "           8       0.50      0.17      0.26        35\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.56       980\n",
      "   macro avg       0.37      0.31      0.32       980\n",
      "weighted avg       0.55      0.56      0.55       980\n",
      "\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_predicted = mlp.predict(X_test)   # use the trained classifier to predict on the test set\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report(y_test, y_predicted))  # compare predictions with ground truth\n",
    "\n",
    "print('\\n')        \n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a468b",
   "metadata": {},
   "source": [
    "#  <font color='red'>Part 4</font>\n",
    "\n",
    "* **Train and tune a different classifier that is not a neural network**\n",
    "\n",
    "We are using Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddcfec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model...\n",
      "Best hyperparameters found on development set for Random Forest:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'n_estimators': 50}\n",
      "f1_score is\n",
      "0.506057317549482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scoring = 'f1_macro'\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "print('Tuning model...')\n",
    "selected_model = RandomForestClassifier()\n",
    "hyperparameters = {'max_depth':[None, 3, 4, 5], 'criterion':['gini', 'entropy'], 'n_estimators':[10, 50] }\n",
    "clf = GridSearchCV(selected_model, hyperparameters, cv=5, scoring=scoring)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters found on development set for Random Forest:\")\n",
    "print(clf.best_params_)\n",
    "tuned_model_RF = clf.best_estimator_\n",
    "\n",
    "y_pred = tuned_model_RF.predict(X_test)\n",
    "print( 'f1_score is')\n",
    "print( f1_score(y_test, y_pred, average='macro') )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe5d6c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++++ Detailed classification report for the selected model ++++ \n",
      "Algorithm: Pipeline(steps=[('randomforestclassifier',\n",
      "                 RandomForestClassifier(criterion='entropy', n_estimators=50))]) \n",
      "This model was trained and tuned on the development set using CV.\n",
      "The following results are computed on the separate test set:\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         5\n",
      "           4       0.57      0.32      0.41        25\n",
      "           5       0.70      0.67      0.68       291\n",
      "           6       0.64      0.77      0.70       432\n",
      "           7       0.74      0.57      0.64       192\n",
      "           8       0.94      0.43      0.59        35\n",
      "\n",
      "    accuracy                           0.67       980\n",
      "   macro avg       0.60      0.46      0.50       980\n",
      "weighted avg       0.68      0.67      0.67       980\n",
      "\n",
      "Cohen Kappa Score: 0.49582417793762\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\mchavoo2\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "seed = 42, \n",
    "\n",
    "# Make predictions on test dataset\n",
    "\n",
    "selected_model = make_pipeline( RandomForestClassifier(criterion= 'entropy', max_depth=None, n_estimators=50) ) \n",
    "selected_model.fit(X_train, y_train)\n",
    "predictions = selected_model.predict(X_test)\n",
    "print(\" ++++ Detailed classification report for the selected model ++++ \" )\n",
    "print(\"Algorithm: %s \" % selected_model)\n",
    "print(\"This model was trained and tuned on the development set using CV.\")\n",
    "print(\"The following results are computed on the separate test set:\")\n",
    "#\n",
    "predictions = selected_model.predict(X_test)\n",
    "\n",
    "#\n",
    "print('\\n clasification report:\\n', classification_report(y_test, predictions))\n",
    "print('Cohen Kappa Score:', cohen_kappa_score(y_test, predictions))\n",
    "print('\\n')        \n",
    "print('done \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199a7bc",
   "metadata": {},
   "source": [
    "Based on accuracy, Random Forest is a better choice than neural networks. \n",
    "\n",
    "Random Forest is the ensemble of decision trees. Each decision tree processes the data and predicts the label. Decision trees in the ensemble are independent, so each can predict the final response. But neural network is made of connected neurons. The neurons cannot operate without other neurons. It process data in each layer and pass forward to the next layers. The last layer of neurons is making decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
